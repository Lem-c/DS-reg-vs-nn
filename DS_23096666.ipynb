{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How can neural network models be used to identify key parameters that lead to road collisions and to predict car accident rates more accurately?\n",
    "\n",
    "### Abstract\n",
    "\n",
    "------\n",
    "\n",
    "## Introduction\n",
    "\n",
    "## Literature review\n",
    "\n",
    "**Deep Belief Networks** (DBN) ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Requirements: Packages used to run the analysis and global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Packages for data manipulation and processing\n",
    "import numpy as np                      # list and matrix calculation\n",
    "import pandas as pd                     # csv file processing\n",
    "\n",
    "# Supervied leraning analysis tools\n",
    "import torch                                    \n",
    "import torch.nn as nn                           # neural network moduls\n",
    "import torch.optim as optim                     # optimizer in NN\n",
    "import torch.nn.functional as F                 # contains a wide range of functions that operate\n",
    "                                                # on tensors and are related to building neural networks\n",
    "from torch.utils.data import TensorDataset, DataLoader # data load related libs\n",
    "from torch.utils.data import Dataset            # training dataset class\n",
    "from torch.utils.data import random_split       # randomly split a dataset into multiple subsets\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder  # encode target value to label\n",
    "from sklearn.model_selection import train_test_split # single split dataset\n",
    "from sklearn.model_selection import KFold       # cross-validation\n",
    "\n",
    "# Data analysis tools\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor  # Calculate vif\n",
    "\n",
    "# Visualization tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns                   # built-in themes to draw attractive and informative statistical graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data relative path\n",
    "collisionDataPath = \"./data/dft-road-casualty-statistics-collision-last-5-years.csv\"\n",
    "casualtyDataPath = \"./data/dft-road-casualty-statistics-casualty-last-5-years.csv\"\n",
    "# dtype: str location indicator of raw data\n",
    "dtype_specification = {0: str, 2: str, 18: str}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data gathering\n",
    "[Ref](https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset class def\n",
    "class TrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, path, loc_remove:list[str], loc_test=None, dtype:dict=None):\n",
    "        \"\"\"\n",
    "        Training dataset initialization method used to convert the data frame from csv file\n",
    "        into list array style that can be used for batch seperation.\n",
    "        \n",
    "        Parameters\n",
    "        ------\n",
    "        path : str, pd.DataFrame\n",
    "            The relative file path of dataset\n",
    "            Can also input data frame directly\n",
    "        loc_remove : list[str]\n",
    "            The columns will be removed\n",
    "        loc_test : list[str], optinal\n",
    "            The index of testing columns\n",
    "        dtype : dict, optional\n",
    "            The specific data type of column(s)\n",
    "        \"\"\"\n",
    "        # load the csv file as a dataframe\n",
    "        if(isinstance(path, pd.DataFrame)):\n",
    "            df = path\n",
    "        else:\n",
    "            df = pd.read_csv(path, dtype=dtype)\n",
    "        # remove useless columns\n",
    "        df = df.drop(columns=loc_remove)\n",
    "        # print first row to preview df\n",
    "        df.head(1)\n",
    "        # stroe the df\n",
    "        self.raw = df\n",
    "        # The training and testing data\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.test = loc_test\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Check the size of dataset.\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        len(self.X) : int\n",
    "            The size of X\n",
    "        \"\"\"\n",
    "        print(f\"Training dataset length: {len(self.X)}, test dataset length: {len(self.y)}\")\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    def get_train_test_split(self):\n",
    "        # store the inputs and outputs\n",
    "        if self.test is None:\n",
    "            self.X = self.raw.values[:, :-1]\n",
    "            self.y = self.raw.values[:, -1]\n",
    "        else:\n",
    "            self.X = self.raw.drop(columns=self.test).to_numpy()\n",
    "            self.y = self.raw[self.test].values.ravel()\n",
    "            # NotImplementedError()\n",
    "        # label encode target\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        # ensure output data is floats\n",
    "        self.y = self.y.astype('float32')\n",
    "        # reshapes output array to have len(self.y) rows and 1 column\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    def reset_raw_data(self, new_df, loc_test=None):\n",
    "        self.raw = new_df\n",
    "        if loc_test is not None:\n",
    "            self.test = loc_test\n",
    "        # Assign X and y\n",
    "        self.get_train_test_split()\n",
    "\n",
    "    def shape(self):\n",
    "        return self.raw.shape\n",
    "\n",
    "    def describe(self, cols: list[str]=None):\n",
    "        \"\"\"\n",
    "        generates descriptive statistics that summarize the df.\n",
    "\n",
    "        Parameter\n",
    "        ------\n",
    "        cols: list[str]\n",
    "            specific clumns that will be described\n",
    "        \"\"\"\n",
    "        if cols is not None:\n",
    "            print(self.raw[cols].describe(include='all'))\n",
    "        else:\n",
    "            print(self.raw.describe(include='all'))\n",
    "\n",
    "    def clean_na(self):\n",
    "        \"\"\"\n",
    "        data processing step. \n",
    "        Remove rows with any null values and replace '-1' with NaN,\n",
    "        then remove any rows with NaN values.\n",
    "        \"\"\"\n",
    "        self.raw = self.raw.dropna()\n",
    "        self.raw = self.raw.replace(-1, pd.NA).dropna()\n",
    "        self.raw = self.raw.replace('-1', pd.NA).dropna()\n",
    "\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        \"\"\"\n",
    "        split the dataset and get indexes for train and test rows.\n",
    "\n",
    "        Parameter\n",
    "        ------\n",
    "        n_test : float\n",
    "            The split ratio of testing dataset [0, 1]\n",
    "        \"\"\"\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure proper rendering of plots\n",
    "%matplotlib inline\n",
    "# # 1. Trend of Accidents Over Years\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.countplot(x='accident_year', data=df)\n",
    "# plt.title('Trend of Accidents Over Years')\n",
    "# plt.ylabel('Number of Accidents')\n",
    "# plt.xlabel('Year')\n",
    "# plt.xticks(rotation=45)\n",
    "# plt.show()\n",
    "\n",
    "# 2. Age Distribution of Casualties\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.histplot(dataSet.raw['age_of_casualty'], bins=30, kde=True)\n",
    "plt.title('Age Distribution of Casualties')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 3. Severity of Accidents\n",
    "plt.figure(figsize=(5, 3))\n",
    "sns.countplot(x='casualty_severity', data=casualtyDataset.raw)\n",
    "plt.title('Severity of Accidents')\n",
    "plt.xlabel('Severity Level')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(ticks=[0, 1, 2], labels=['Slight', 'Serious', 'Fatal'])\n",
    "plt.show()\n",
    "\n",
    "# 4. Casualty Type Distribution\n",
    "plt.figure(figsize=(6, 3))\n",
    "# Count the frequency of each casualty type\n",
    "casualty_type_counts = casualtyDataset.raw['casualty_type'].value_counts().sort_values(ascending=False)\n",
    "# Plot as a bar chart\n",
    "sns.barplot(x=casualty_type_counts.values, y=casualty_type_counts.index, orient='h')\n",
    "plt.title('Distribution of Casualty Types')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Casualty Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "\n",
    "Checking the note in the data readme file, -1 means there is a data fail to be recored or out of the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "casualtyDataset = TrainDataset(casualtyDataPath, ['accident_index', 'accident_year', \n",
    "       'vehicle_reference', 'casualty_reference', 'car_passenger', 'bus_or_coach_passenger',\n",
    "       'pedestrian_road_maintenance_worker', 'lsoa_of_casualty'],\n",
    "             dtype=dtype_specification)\n",
    "collisionDataset = TrainDataset(collisionDataPath, ['accident_index', 'accident_year',\n",
    "       'location_easting_osgr', 'location_northing_osgr', 'longitude',\n",
    "       'latitude', 'police_force', 'date', 'time', 'local_authority_district',\n",
    "       'local_authority_ons_district', 'local_authority_highway', 'first_road_number', \n",
    "       'second_road_number', 'did_police_officer_attend_scene_of_accident',\n",
    "       'lsoa_of_accident_location'],\n",
    "       dtype=dtype_specification)\n",
    "\n",
    "casualtyDataset.clean_na()\n",
    "collisionDataset.clean_na()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair: (1, 1), Value: 0\n",
      "Pair: (1, 2), Value: 1\n",
      "Pair: (1, 3), Value: 2\n",
      "Pair: (1, 6), Value: 3\n",
      "Pair: (1, 7), Value: 4\n",
      "Pair: (1, 9), Value: 5\n",
      "Pair: (1, 12), Value: 6\n",
      "Pair: (2, 1), Value: 7\n",
      "Pair: (2, 2), Value: 8\n",
      "Pair: (2, 3), Value: 9\n",
      "Pair: (2, 6), Value: 10\n",
      "Pair: (2, 7), Value: 11\n",
      "Pair: (2, 9), Value: 12\n",
      "Pair: (2, 12), Value: 13\n",
      "Pair: (3, 1), Value: 14\n",
      "Pair: (3, 2), Value: 15\n",
      "Pair: (3, 3), Value: 16\n",
      "Pair: (3, 6), Value: 17\n",
      "Pair: (3, 7), Value: 18\n",
      "Pair: (3, 9), Value: 19\n",
      "Pair: (3, 12), Value: 20\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The target columns are: 'casualty_severity', 'road_type'\n",
    "However, the y column that used for prediction has to be one dismension.\n",
    "Therefor, a look up table that saves the combination of these two columns has\n",
    "to be created.\n",
    "\"\"\"\n",
    "# Define the pairs [1, 2, 3] [1, 2, 3, 6, 7, 9, 12]\n",
    "pairs = [(1, 1), (0, 3), (0, 4), (0, 7), (1, 1), (1, 3), (1, 7), (2, 7)]\n",
    "\n",
    "def get_look_up_table(list1, list2):\n",
    "    lookup_table = {}\n",
    "    counter = 0\n",
    "    # Map each unique pair to a unique integer, incrementing sequentially.\n",
    "    for item1 in list1:\n",
    "        for item2 in list2:\n",
    "            lookup_table[(item1, item2)] = counter\n",
    "            counter += 1\n",
    "    return lookup_table\n",
    "\n",
    "lookup_table = get_look_up_table([1, 2, 3], [1, 2, 3, 6, 7, 9, 12])\n",
    "# Print the lookup table\n",
    "for pair, value in lookup_table.items():\n",
    "    print(f\"Pair: {pair}, Value: {value}\")\n",
    "\n",
    "# Apply the lookup table to each row\n",
    "def lookup_value(row, class1='accident_severity', class2='road_type'):\n",
    "    try:\n",
    "        return lookup_table[(row[class1], row[class2])]\n",
    "    except KeyError as e:\n",
    "        print(f\"KeyError for row: {row.to_dict()}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accident_reference                             0\n",
      "casualty_class                             23623\n",
      "sex_of_casualty                            23623\n",
      "age_of_casualty                            23623\n",
      "age_band_of_casualty                       23623\n",
      "casualty_severity                          23623\n",
      "pedestrian_location                        23623\n",
      "pedestrian_movement                        23623\n",
      "casualty_type                              23623\n",
      "casualty_home_area_type                    23623\n",
      "casualty_imd_decile                        23623\n",
      "accident_severity                              0\n",
      "number_of_vehicles                             0\n",
      "number_of_casualties                           0\n",
      "day_of_week                                    0\n",
      "first_road_class                               0\n",
      "road_type                                      0\n",
      "speed_limit                                    0\n",
      "junction_detail                                0\n",
      "junction_control                               0\n",
      "second_road_class                              0\n",
      "pedestrian_crossing_human_control              0\n",
      "pedestrian_crossing_physical_facilities        0\n",
      "light_conditions                               0\n",
      "weather_conditions                             0\n",
      "road_surface_conditions                        0\n",
      "special_conditions_at_site                     0\n",
      "carriageway_hazards                            0\n",
      "urban_or_rural_area                            0\n",
      "trunk_road_flag                                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Merge the casulty and collision data set together by right\n",
    "collision_casualty_merged = pd.merge(casualtyDataset.raw, collisionDataset.raw, how='right', on='accident_reference')\n",
    "# Check how many null values existed in merged result\n",
    "print(collision_casualty_merged.isnull().sum()) \n",
    "# Appl look up table to combine two classes together\n",
    "collision_casualty_merged['labels'] = collision_casualty_merged.apply(lookup_value, axis=1)\n",
    "# Get final dataset used for trainning\n",
    "dataSet = TrainDataset(collision_casualty_merged, loc_remove=['accident_reference', 'casualty_severity',\n",
    "                                                            'accident_severity', 'road_type'],\n",
    "                       loc_test=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Null and invalid value\n",
    "dataSet.clean_na()\n",
    "dataSet.get_train_test_split()\n",
    "\n",
    "print(dataSet.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vif(df):\n",
    "    # Calculating VIF for each independent variable\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['feature'] = df.columns\n",
    "    vif_data['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "\n",
    "    print(vif_data)\n",
    "    return vif_data\n",
    "\n",
    "def remove_multicollinear_columns(df, vif_threshold=10.0):\n",
    "    \"\"\"\n",
    "    Removes columns from the dataframe df that have a variance inflation factor (VIF) greater than the threshold.\n",
    "    \"\"\"\n",
    "    # Calculate VIF for each feature\n",
    "    vif_data = calculate_vif(df)\n",
    "    \n",
    "    # Find features with VIF exceeding the threshold\n",
    "    features_to_drop = vif_data[vif_data['VIF'] > vif_threshold]['feature']\n",
    "    \n",
    "    # Drop the features from the original dataframe\n",
    "    df_reduced = df.drop(columns=features_to_drop)\n",
    "    \n",
    "    return df_reduced, features_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    feature         VIF\n",
      "0                            casualty_class   13.383351\n",
      "1                           sex_of_casualty    9.602845\n",
      "2                           age_of_casualty  102.888245\n",
      "3                      age_band_of_casualty  197.153435\n",
      "4                       pedestrian_location    4.366444\n",
      "5                       pedestrian_movement    3.129670\n",
      "6                             casualty_type    1.655347\n",
      "7                   casualty_home_area_type    6.927835\n",
      "8                       casualty_imd_decile    4.443632\n",
      "9                        number_of_vehicles   13.968046\n",
      "10                     number_of_casualties    3.937885\n",
      "11                              day_of_week    5.493072\n",
      "12                         first_road_class   12.428804\n",
      "13                              speed_limit   16.930174\n",
      "14                          junction_detail    1.724037\n",
      "15                         junction_control   14.299633\n",
      "16                        second_road_class   21.590626\n",
      "17        pedestrian_crossing_human_control    1.711400\n",
      "18  pedestrian_crossing_physical_facilities    1.897074\n",
      "19                         light_conditions    2.608823\n",
      "20                       weather_conditions    2.171507\n",
      "21                  road_surface_conditions    4.360007\n",
      "22               special_conditions_at_site    1.772098\n",
      "23                      carriageway_hazards    1.816165\n",
      "24                      urban_or_rural_area   17.914478\n",
      "25                          trunk_road_flag   69.420604\n"
     ]
    }
   ],
   "source": [
    "df_raw = dataSet.raw.drop('labels', axis=1)\n",
    "# convert the data into numeric fromat to ensure the correct process\n",
    "df_raw = df_raw.apply(pd.to_numeric, errors='coerce')\n",
    "# get reduced df after appplying VIF remove\n",
    "df_raw_fited, _ = remove_multicollinear_columns(df_raw, 15)\n",
    "# add the dropped Y label column back again\n",
    "df_raw_fited['labels'] = dataSet.raw['labels']\n",
    "# re-assign the df back to object\n",
    "dataSet.reset_raw_data(df_raw_fited, loc_test=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>casualty_class</th>\n",
       "      <th>sex_of_casualty</th>\n",
       "      <th>pedestrian_location</th>\n",
       "      <th>pedestrian_movement</th>\n",
       "      <th>casualty_type</th>\n",
       "      <th>casualty_home_area_type</th>\n",
       "      <th>casualty_imd_decile</th>\n",
       "      <th>number_of_vehicles</th>\n",
       "      <th>number_of_casualties</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>junction_detail</th>\n",
       "      <th>junction_control</th>\n",
       "      <th>pedestrian_crossing_human_control</th>\n",
       "      <th>pedestrian_crossing_physical_facilities</th>\n",
       "      <th>light_conditions</th>\n",
       "      <th>weather_conditions</th>\n",
       "      <th>road_surface_conditions</th>\n",
       "      <th>special_conditions_at_site</th>\n",
       "      <th>carriageway_hazards</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   casualty_class  sex_of_casualty  pedestrian_location  pedestrian_movement  \\\n",
       "0             3.0                1                    5                    1   \n",
       "1             1.0                1                    0                    0   \n",
       "2             1.0                1                    0                    0   \n",
       "3             1.0                1                    0                    0   \n",
       "4             1.0                2                    0                    0   \n",
       "5             1.0                1                    0                    0   \n",
       "\n",
       "   casualty_type  casualty_home_area_type  casualty_imd_decile  \\\n",
       "0              0                        1                    3   \n",
       "1              8                        1                    3   \n",
       "2              9                        1                    7   \n",
       "3              1                        1                    3   \n",
       "4              8                        1                    3   \n",
       "5              3                        1                    6   \n",
       "\n",
       "   number_of_vehicles  number_of_casualties  day_of_week  ...  \\\n",
       "0                   1                     1            2  ...   \n",
       "1                   2                     1            2  ...   \n",
       "2                   2                     1            2  ...   \n",
       "3                   2                     1            2  ...   \n",
       "4                   3                     1            2  ...   \n",
       "5                   2                     1            2  ...   \n",
       "\n",
       "   junction_detail  junction_control  pedestrian_crossing_human_control  \\\n",
       "0                2                 4                                  0   \n",
       "1                6                 4                                  0   \n",
       "2                7                 2                                  0   \n",
       "3                2                 4                                  0   \n",
       "4                5                 4                                  0   \n",
       "5                3                 4                                  0   \n",
       "\n",
       "   pedestrian_crossing_physical_facilities  light_conditions  \\\n",
       "0                                        0                 4   \n",
       "1                                        5                 4   \n",
       "2                                        5                 4   \n",
       "3                                        0                 4   \n",
       "4                                        0                 7   \n",
       "5                                        8                 4   \n",
       "\n",
       "   weather_conditions  road_surface_conditions  special_conditions_at_site  \\\n",
       "0                   1                        1                           0   \n",
       "1                   1                        1                           0   \n",
       "2                   2                        2                           0   \n",
       "3                   1                        1                           0   \n",
       "4                   1                        1                           0   \n",
       "5                   1                        1                           0   \n",
       "\n",
       "   carriageway_hazards  labels  \n",
       "0                    0      17  \n",
       "1                    0      17  \n",
       "2                    0       9  \n",
       "3                    0      10  \n",
       "4                    0      15  \n",
       "5                    0      17  \n",
       "\n",
       "[6 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dataSet.raw.head(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (327408, 20)\n",
      "Testing data shape: (327408, 1)\n"
     ]
    }
   ],
   "source": [
    "# Data split into training and testing sets with a 7:3 ratio\n",
    "train_X, test_X, train_y, test_y = train_test_split(dataSet.X, dataSet.y, test_size=0.3, random_state=2024)\n",
    "\n",
    "print(f\"Training data shape: {dataSet.X.shape}\")\n",
    "print(f\"Testing data shape: {dataSet.y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:               327408\n",
      "Model:                            GLM   Df Residuals:                   327387\n",
      "Model Family:                 Poisson   Df Model:                           20\n",
      "Link Function:                    Log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:            -8.2510e+05\n",
      "Date:                Fri, 12 Apr 2024   Deviance:                   2.0556e+05\n",
      "Time:                        11:15:09   Pearson chi2:                 1.79e+05\n",
      "No. Iterations:                     4   Pseudo R-squ. (CS):            0.02308\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.5142      0.004    635.327      0.000       2.506       2.522\n",
      "x1            -0.0038      0.001     -3.421      0.001      -0.006      -0.002\n",
      "x2             0.0312      0.001     31.065      0.000       0.029       0.033\n",
      "x3            -0.0049      0.000     -9.782      0.000      -0.006      -0.004\n",
      "x4             0.0003      0.000      0.731      0.465      -0.001       0.001\n",
      "x5             0.0002   5.08e-05      3.937      0.000       0.000       0.000\n",
      "x6            -0.0081      0.001     -9.110      0.000      -0.010      -0.006\n",
      "x7            -0.0020      0.000    -11.314      0.000      -0.002      -0.002\n",
      "x8             0.0172      0.001     18.899      0.000       0.015       0.019\n",
      "x9            -0.0177      0.001    -35.078      0.000      -0.019      -0.017\n",
      "x10            0.0003      0.000      1.239      0.215      -0.000       0.001\n",
      "x11            0.0105      0.000     29.243      0.000       0.010       0.011\n",
      "x12            0.0010   3.93e-05     24.663      0.000       0.001       0.001\n",
      "x13           -0.0007      0.000     -1.551      0.121      -0.002       0.000\n",
      "x14            0.0057      0.000     17.058      0.000       0.005       0.006\n",
      "x15            0.0009      0.000      4.410      0.000       0.001       0.001\n",
      "x16           -0.0038      0.000    -12.630      0.000      -0.004      -0.003\n",
      "x17            0.0026      0.000      8.916      0.000       0.002       0.003\n",
      "x18            0.0032      0.001      5.385      0.000       0.002       0.004\n",
      "x19            0.0019      0.000      4.143      0.000       0.001       0.003\n",
      "x20            0.0027      0.001      5.070      0.000       0.002       0.004\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "sm_X = sm.add_constant(dataSet.X)\n",
    "# Create the GLM model - assuming a Poisson family since 'labels' seems to be a count\n",
    "# For other types of data, you may choose a different family like sm.families.Gaussian() for continuous data\n",
    "glm_model = sm.GLM(dataSet.y, sm_X, family=sm.families.Poisson())\n",
    "\n",
    "# Fit the model\n",
    "glm_results = glm_model.fit()\n",
    "\n",
    "# Summary of model\n",
    "print(glm_results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is: 2.16%\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = glm_results.predict(sm_X[1000, :])  # Predictions on the training set\n",
    "# Convert predictions to the same scale as your labels if necessary\n",
    "predicted_labels = np.rint(predictions)  # Round predictions to the nearest integer\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = np.mean(predicted_labels == dataSet.y)  # Comparing predicted labels with true labels\n",
    "print(f\"The accuracy of the model is: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network models\n",
    "class Modle_1(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Modle_1, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, num_classes)\n",
    "        # No softmax here, CrossEntropyLoss will handle that\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.layer3(x)  # Outputs are logits\n",
    "        return x\n",
    "    \n",
    "class Modle_2(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Modle_2, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 256)  # Increased layer size\n",
    "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout layer\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.batch_norm3 = nn.BatchNorm1d(64)\n",
    "        self.layer4 = nn.Linear(64, num_classes)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)  # LeakyReLU activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.batch_norm1(self.layer1(x)))\n",
    "        x = self.leaky_relu(self.batch_norm2(self.layer2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.leaky_relu(self.batch_norm3(self.layer3(x)))\n",
    "        x = self.layer4(x)  # Outputs are logits\n",
    "        return x\n",
    "    \n",
    "class Model_3(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(Model_3, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in train_y: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])\n",
      "20\n",
      "Start training...\n",
      "epoch: 0, Loss: 1.19113290309906\n",
      "epoch: 1, Loss: 1.1428592205047607\n",
      "epoch: 2, Loss: 0.9452615976333618\n",
      "epoch: 3, Loss: 0.9381357431411743\n",
      "epoch: 4, Loss: 1.1191359758377075\n",
      "Accuracy: 61.46%\n",
      "Predicted classes: [15 15 15 12 15]\n"
     ]
    }
   ],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(train_X.astype('float32'), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(test_X.astype('float32'), dtype=torch.float32)\n",
    "y_train_tensor = torch.squeeze(torch.tensor(train_y, dtype=torch.long))\n",
    "y_test_tensor = torch.squeeze(torch.tensor(test_y, dtype=torch.long))\n",
    "\n",
    "print(\"Unique labels in train_y:\", torch.unique(y_train_tensor))\n",
    "\n",
    "# Define the dataset and dataloader\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "input_size = train_X.shape[1]\n",
    "print(input_size)\n",
    "# The number of classes\n",
    "num_classes = 21\n",
    "\n",
    "model = Model_3(input_size=input_size, num_classes=num_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Start training...\")\n",
    "# Train the model\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    batch_counter = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        if inputs.size(0) > 1:  # Check if batch size is greater than 1\n",
    "            inputs, targets = inputs.float(), targets.long()  # Ensure correct types\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            # print(\"Encountered batch size of 1, skipping...\")\n",
    "            continue\n",
    "            \n",
    "    # print current training state    \n",
    "    print(f\"epoch: {epoch}, Loss: {loss.item()}\")    \n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Make predictions (on unseen data)\n",
    "with torch.no_grad():\n",
    "    sample_data = X_test_tensor[:5]\n",
    "    sample_preds = model(sample_data)\n",
    "    _, predicted_classes = torch.max(sample_preds, 1)\n",
    "print(f'Predicted classes: {predicted_classes.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.38%\n"
     ]
    }
   ],
   "source": [
    "model_loaded = torch.load('model_1.pth')\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model_loaded.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model_loaded(X_test_tensor)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    accuracy = (predicted == y_test_tensor).sum().item() / y_test_tensor.size(0)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For cross-validation, KFold or StratifiedKFold from sklearn.model_selection, which splits the dataset into k consecutive folds, will be used. Each fold is then used once as a validation while the $k-1$ remaining folds form the training set. This process is repeated $k$ times (folds), with each of the $k$ folds used exactly once as the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [     0      1      2 ... 538456 538459 538460] Test: [     4      6      7 ... 538448 538457 538458]\n",
      "Train: [     1      3      4 ... 538457 538458 538460] Test: [     0      2      5 ... 538453 538455 538459]\n",
      "Train: [     0      2      3 ... 538458 538459 538460] Test: [     1      8     13 ... 538441 538447 538454]\n",
      "Train: [     0      1      2 ... 538457 538458 538459] Test: [     3     14     16 ... 538450 538451 538460]\n",
      "Train: [     0      1      2 ... 538458 538459 538460] Test: [     9     10     19 ... 538438 538439 538456]\n"
     ]
    }
   ],
   "source": [
    "# Setup cross-validation with 5 folds\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2024)\n",
    "\n",
    "# Example: Print the indices of the data for each fold\n",
    "for train_index, test_index in kf.split(dataset.X):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    X_train, X_test = dataset.X[train_index], dataset.X[test_index]\n",
    "    y_train, y_test = dataset.y[train_index], dataset.y[test_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
